{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f199cf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:43:46.224970Z",
     "iopub.status.busy": "2023-11-07T22:43:46.224124Z",
     "iopub.status.idle": "2023-11-07T22:43:47.105035Z",
     "shell.execute_reply": "2023-11-07T22:43:47.103964Z"
    },
    "id": "kKt9euepQTpp",
    "papermill": {
     "duration": 0.892096,
     "end_time": "2023-11-07T22:43:47.107308",
     "exception": false,
     "start_time": "2023-11-07T22:43:46.215212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/project/jacobcha/nk643/gans/fashion-image-generator.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bn0007/project/jacobcha/nk643/gans/fashion-image-generator.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m \u001b[39m# linear algebra\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bn0007/project/jacobcha/nk643/gans/fashion-image-generator.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \u001b[39m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bn0007/project/jacobcha/nk643/gans/fashion-image-generator.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Input data files are available in the read-only \"../input/\" directory\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bn0007/project/jacobcha/nk643/gans/fashion-image-generator.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# For example, running this (by clicking run or pressing Shift+Enter) will list all files u\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files u\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f27c78",
   "metadata": {
    "id": "j_LKoKrxQZ6f",
    "papermill": {
     "duration": 0.007024,
     "end_time": "2023-11-07T22:43:47.121630",
     "exception": false,
     "start_time": "2023-11-07T22:43:47.114606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3a9352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:43:47.137782Z",
     "iopub.status.busy": "2023-11-07T22:43:47.137012Z",
     "iopub.status.idle": "2023-11-07T22:43:53.814582Z",
     "shell.execute_reply": "2023-11-07T22:43:53.813343Z"
    },
    "papermill": {
     "duration": 6.687456,
     "end_time": "2023-11-07T22:43:53.816201",
     "exception": true,
     "start_time": "2023-11-07T22:43:47.128745",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Convert the features and labels into TensorFlow tensors\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(labels, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# One-hot encode the labels\u001b[39;00m\n\u001b[1;32m     11\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Reshape the features for a 28x28 image with 1 color channel\u001b[39;00m\n\u001b[1;32m     12\u001b[0m features \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(features, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "data_train_file = \"/kaggle/input/fashionmnist/fashion-mnist_train.csv\"\n",
    "df = pd.read_csv(data_train_file)\n",
    "\n",
    "# Assuming the first column is the label and the rest are pixel values\n",
    "labels = df['label'].values\n",
    "features = df.drop('label', axis=1).values\n",
    "\n",
    "# Convert the features and labels into TensorFlow tensors\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=10)  # One-hot encode the labels\n",
    "features = features.reshape((-1, 28, 28, 1))  # Reshape the features for a 28x28 image with 1 color channel\n",
    "features = tf.convert_to_tensor(features, dtype=tf.float32)\n",
    "labels = tf.convert_to_tensor(labels, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c7cf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:08:08.686956Z",
     "iopub.status.busy": "2023-11-07T22:08:08.686088Z",
     "iopub.status.idle": "2023-11-07T22:08:57.839168Z",
     "shell.execute_reply": "2023-11-07T22:08:57.838199Z",
     "shell.execute_reply.started": "2023-11-07T22:08:08.686922Z"
    },
    "id": "ugiWn0y4Qhkp",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages (only need to do this once)\n",
    "# !pip install tensorflow tensorflow-gpu matplotlib tensorflow-datasets ipywidgets\n",
    "# !pip list\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D\n",
    "import tensorflow_datasets as tfds\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Configure TensorFlow to use GPU for faster computation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "ds = tfds.load('fashion_mnist', split='train')\n",
    "#data_train_file = \"/kaggle/input/fashionmnist/fashion-mnist_train.csv\"\n",
    "# data_test_file = \"../input/fashion-mnist_test.csv\"\n",
    "\n",
    "#ds= pd.read_csv(data_train_file)\n",
    "# df_test = pd.read_csv(data_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a42e0d",
   "metadata": {
    "id": "2OOgzoFJRQUm",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Visualize Data and Build a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8b4f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:09:27.859383Z",
     "iopub.status.busy": "2023-11-07T22:09:27.858975Z",
     "iopub.status.idle": "2023-11-07T22:09:32.067344Z",
     "shell.execute_reply": "2023-11-07T22:09:32.066259Z",
     "shell.execute_reply.started": "2023-11-07T22:09:27.859349Z"
    },
    "id": "RX_t0VuVQslf",
    "outputId": "b02b9877-8bc1-414a-f61e-3ea5d2e27718",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image\n",
    "# Data Transformation: Scale and Vizualize Images\n",
    "import numpy as np\n",
    "\n",
    "# Setup data iterator\n",
    "dataiterator = ds.as_numpy_iterator()\n",
    "\n",
    "# Visualize some images from the dataset\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "\n",
    "# Loop four times and get images\n",
    "for idx in range(4):\n",
    "    # Grab an image and its label\n",
    "    sample = dataiterator.next()\n",
    "    image = np.squeeze(sample['image'])  # Remove the single-dimensional entries\n",
    "    label = sample['label']\n",
    "\n",
    "    # Plot the image using a specific subplot\n",
    "    ax[idx].imshow(image)\n",
    "    ax[idx].title.set_text(label)\n",
    "\n",
    "# Data Preprocessing: Scale and Batch the Images\n",
    "def scale_images(data):\n",
    "    # Scale the pixel values of the images between 0 and 1\n",
    "    image = data['image']\n",
    "    # return image / 255.0\n",
    "    return (tf.cast(image, tf.float32) / 255.0)\n",
    "\n",
    "\n",
    "# Reload the dataset\n",
    "# ds = tfds.load('fashion_mnist', split='train')\n",
    "\n",
    "# Apply the scale_images preprocessing step to the dataset\n",
    "ds = ds.map(scale_images)\n",
    "\n",
    "# Cache the dataset for faster processing during training\n",
    "ds = ds.cache()\n",
    "\n",
    "# Shuffle the dataset to add randomness to the training process\n",
    "ds = ds.shuffle(60000)\n",
    "\n",
    "# Batch the dataset into smaller groups (128 images per batch)\n",
    "ds = ds.batch(128)\n",
    "\n",
    "# Prefetch the dataset to improve performance during training\n",
    "ds = ds.prefetch(64)\n",
    "\n",
    "# Check the shape of a batch of images\n",
    "ds.as_numpy_iterator().next().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fb8a5",
   "metadata": {
    "id": "H8NPWFV0j-eh",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Build the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44460359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:09:37.601315Z",
     "iopub.status.busy": "2023-11-07T22:09:37.599571Z",
     "iopub.status.idle": "2023-11-07T22:09:37.605750Z",
     "shell.execute_reply": "2023-11-07T22:09:37.604722Z",
     "shell.execute_reply.started": "2023-11-07T22:09:37.601283Z"
    },
    "id": "dxkyIu_wRcwP",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the Sequential API for building models\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Import the layers required for the neural network\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a5586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:09:37.861250Z",
     "iopub.status.busy": "2023-11-07T22:09:37.860871Z",
     "iopub.status.idle": "2023-11-07T22:09:38.070506Z",
     "shell.execute_reply": "2023-11-07T22:09:38.069545Z",
     "shell.execute_reply.started": "2023-11-07T22:09:37.861208Z"
    },
    "id": "YTAP4cpskBw5",
    "outputId": "6beb887f-026d-4aca-d148-9d4ca415908d",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    model = Sequential()\n",
    "\n",
    "    # First layer takes random noise and reshapes it to 7x7x128\n",
    "    # This is the beginning of the generated image\n",
    "    model.add(Dense(7 * 7 * 128, input_dim=128))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "\n",
    "    # Upsampling block 1\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 5, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    # Upsampling block 2\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 5, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    # Convolutional block 1\n",
    "    model.add(Conv2D(128, 4, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    # Convolutional block 2\n",
    "    model.add(Conv2D(128, 4, padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    # Convolutional layer to get to one channel\n",
    "    model.add(Conv2D(1, 4, padding='same', activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the generator model\n",
    "generator = build_generator()\n",
    "# Display the model summary\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebbd80",
   "metadata": {
    "id": "HEuuvzqJkPbT",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Build the Discriminatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4615bfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:09:42.248540Z",
     "iopub.status.busy": "2023-11-07T22:09:42.248147Z",
     "iopub.status.idle": "2023-11-07T22:09:42.394571Z",
     "shell.execute_reply": "2023-11-07T22:09:42.393598Z",
     "shell.execute_reply.started": "2023-11-07T22:09:42.248509Z"
    },
    "id": "rJTSkL7mkKTT",
    "outputId": "16c79cb0-a89c-4a31-c22e-5abb1b7bfd17",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(32, 5, input_shape=(28, 28, 1)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(64, 5))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    model.add(Conv2D(128, 5))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Fourth Convolutional Block\n",
    "    model.add(Conv2D(256, 5))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    # Flatten the output and pass it through a dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the discriminator model\n",
    "discriminator = build_discriminator()\n",
    "# Display the model summary\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d74c0",
   "metadata": {
    "id": "G_mnyyUHkcx3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Construct the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d3cb7",
   "metadata": {
    "id": "QUQU9-uBkfbq",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Set up Losses and Optimizer\n",
    "Before building the training loop, we need to define the loss functions and optimizers that will be used to train both the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39bf8e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:09:46.046531Z",
     "iopub.status.busy": "2023-11-07T22:09:46.045700Z",
     "iopub.status.idle": "2023-11-07T22:09:46.058031Z",
     "shell.execute_reply": "2023-11-07T22:09:46.057224Z",
     "shell.execute_reply.started": "2023-11-07T22:09:46.046495Z"
    },
    "id": "Huo-l_6QkbCq",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the Adam optimizer and Binary Cross Entropy loss function\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Define the optimizers for the generator and discriminator\n",
    "g_opt = Adam(learning_rate=0.0001)  # Generator optimizer\n",
    "d_opt = Adam(learning_rate=0.00001)  # Discriminator optimizer\n",
    "\n",
    "# Define the loss functions for the generator and discriminator\n",
    "g_loss = BinaryCrossentropy()  # Generator loss function\n",
    "d_loss = BinaryCrossentropy()  # Discriminator loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d5d83",
   "metadata": {
    "id": "tVYYRweQkrAu",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Build Subclassed Model\n",
    "Next, we will build a subclassed model that combines the generator and discriminator models into a single GAN model. This subclassed model will train the GAN during the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223618e1",
   "metadata": {
    "id": "tCTHBd5ElE3B",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Build Callback\n",
    "Callbacks in TensorFlow are functions that can be executed during training at specific points, such as the end of an epoch. We will create a custom callback called ModelMonitor to generate and save images at the end of each epoch to monitor the progress of the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6996dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:09:47.882902Z",
     "iopub.status.busy": "2023-11-07T22:09:47.882208Z",
     "iopub.status.idle": "2023-11-07T22:09:47.914081Z",
     "shell.execute_reply": "2023-11-07T22:09:47.913158Z",
     "shell.execute_reply.started": "2023-11-07T22:09:47.882862Z"
    },
    "id": "C8JVjdh-kjgk",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class FashionGAN(Model):\n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        # Pass through args and kwargs to the base class\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Create attributes for generator and discriminator models\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs):\n",
    "        # Compile with the base class\n",
    "        super().compile(*args, **kwargs)\n",
    "\n",
    "        # Create attributes for optimizers and loss functions\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss\n",
    "    def train_step(self, batch):\n",
    "        # Get the data for real images\n",
    "        real_images = batch\n",
    "        # Generate fake images using the generator with random noise as input\n",
    "        fake_images = self.generator(tf.random.normal((128, 128, 1)), training=False)\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            # Pass real and fake images through the discriminator model\n",
    "            yhat_real = self.discriminator(real_images, training=True)\n",
    "            yhat_fake = self.discriminator(fake_images, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "\n",
    "            # Create labels for real and fake images\n",
    "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
    "\n",
    "            # Add some noise to the true outputs to make training more robust\n",
    "            noise_real = 0.15 * tf.random.uniform(tf.shape(yhat_real))\n",
    "            noise_fake = -0.15 * tf.random.uniform(tf.shape(yhat_fake))\n",
    "            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n",
    "           # Calculate the total discriminator loss\n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "\n",
    "        # Apply backpropagation and update discriminator weights\n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            # Generate new images using the generator with random noise as input\n",
    "            gen_images = self.generator(tf.random.normal((128, 128, 1)), training=True)\n",
    "\n",
    "            # Create the predicted labels (should be close to 1 as they are fake images)\n",
    "            predicted_labels = self.discriminator(gen_images, training=False)\n",
    "\n",
    "            # Calculate the total generator loss (tricking the discriminator to classify the fake images as real)\n",
    "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels)\n",
    "\n",
    "        # Apply backpropagation and update generator weights\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "\n",
    "        return {\"d_loss\": total_d_loss, \"g_loss\": total_g_loss}\n",
    "# Create an instance of the FashionGAN model\n",
    "fashgan = FashionGAN(generator, discriminator)\n",
    "\n",
    "# Compile the model with the optimizers and loss functions\n",
    "fashgan.compile(g_opt, d_opt, g_loss, d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ecc71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:14:43.701516Z",
     "iopub.status.busy": "2023-11-07T22:14:43.701094Z",
     "iopub.status.idle": "2023-11-07T22:14:43.709273Z",
     "shell.execute_reply": "2023-11-07T22:14:43.708357Z",
     "shell.execute_reply.started": "2023-11-07T22:14:43.701482Z"
    },
    "id": "ZfyR2OTZlAQ2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ModelMonitor(Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Generate random latent vectors as input to the generator\n",
    "        random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim, 1))\n",
    "        # Generate fake images using the generator\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            # Save the generated images to disk\n",
    "            img = array_to_img(generated_images[i])\n",
    "            img.save(os.path.join('/kaggle/working/images/', f'generated_img_{epoch}_{i}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6f880",
   "metadata": {
    "id": "O0Tcjib8lOnS",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Train the GAN\n",
    "Now that we have set up the GAN model and the custom callback, we can start the training process using the fit method. We will train the GAN for sufficient epochs to allow the generator and discriminator to converge and learn from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f06b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-07T22:40:40.011861Z",
     "iopub.status.busy": "2023-11-07T22:40:40.010998Z",
     "iopub.status.idle": "2023-11-07T22:41:40.700649Z",
     "shell.execute_reply": "2023-11-07T22:41:40.699260Z",
     "shell.execute_reply.started": "2023-11-07T22:40:40.011827Z"
    },
    "id": "Ery4K0CxlLG0",
    "outputId": "ecd30e2e-352f-44eb-f217-be066b71aa54",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the GAN model\n",
    "hist = fashgan.fit(ds, epochs=20, callbacks=[ModelMonitor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c548a1",
   "metadata": {
    "id": "8WDqRE7KlbYJ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Review Performance and Test the Generator\n",
    "## Review performance\n",
    "After training the GAN, we can review its performance by plotting the discriminator and generator losses over the training epochs. This will help us understand how well the GAN has learned and whether there are any issues, such as mode collapse or unstable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559fe95",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-07T22:11:29.666454Z",
     "iopub.status.idle": "2023-11-07T22:11:29.666794Z",
     "shell.execute_reply": "2023-11-07T22:11:29.666642Z",
     "shell.execute_reply.started": "2023-11-07T22:11:29.666627Z"
    },
    "id": "jD8qi0MflR6B",
    "outputId": "24b56bab-e5ed-4ec2-843b-8a46122f8457",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the discriminator and generator losses\n",
    "plt.suptitle('Loss')\n",
    "plt.plot(hist.history['d_loss'], label='d_loss')\n",
    "plt.plot(hist.history['g_loss'], label='g_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f3eeb",
   "metadata": {
    "id": "s6TyJh0jlkl-",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5388d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-07T22:11:29.668173Z",
     "iopub.status.idle": "2023-11-07T22:11:29.668532Z",
     "shell.execute_reply": "2023-11-07T22:11:29.668387Z",
     "shell.execute_reply.started": "2023-11-07T22:11:29.668370Z"
    },
    "id": "YFJIAA3dlgiz",
    "outputId": "a9207574-45a2-436f-ce64-96b58705df15",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99e1aeb6",
   "metadata": {
    "id": "dZ9_JO7VlsMC",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Save the Model\n",
    "Finally, if you are satisfied with the performance of your GAN, you can save the generator and discriminator models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1de63c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-07T22:11:29.670064Z",
     "iopub.status.idle": "2023-11-07T22:11:29.670412Z",
     "shell.execute_reply": "2023-11-07T22:11:29.670257Z",
     "shell.execute_reply.started": "2023-11-07T22:11:29.670223Z"
    },
    "id": "ioAIeMD0loK7",
    "outputId": "7fdd4526-5128-4c5e-ae7c-71c74d39631f",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the generator and discriminator models\n",
    "generator.save('/kaggle/working/gan_fas/generator.h5')\n",
    "discriminator.save('/kaggle/working/gan_fas/discriminator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa8e11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Test out the Generator\n",
    "After training the GAN and reviewing its performance, we can test the generator by generating and visualizing new fashion images. First, we will load the weights of the trained generator and use it to generate new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba4356",
   "metadata": {
    "id": "PAaJeImAlvCj",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the weights of the trained generator\n",
    "generator.load_weights('/kaggle/working/gen/generator.h5')\n",
    "\n",
    "# Generate new fashion images\n",
    "imgs = generator.predict(tf.random.normal((16, 128, 1)))\n",
    "\n",
    "# Plot the generated images\n",
    "fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10, 10))\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        ax[r][c].imshow(imgs[(r + 1) * (c + 1) - 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.915469,
   "end_time": "2023-11-07T22:43:54.241343",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-07T22:43:42.325874",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
